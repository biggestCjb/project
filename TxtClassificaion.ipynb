{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simbert不能正常使用，除非你安装：bert4keras、tensorflow ，为了安装快捷，没有默认安装.... No module named 'bert4keras'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from nlpcda import Similarword\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, csv_path, type, is_augement):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df = self.df.dropna()\n",
    "        # if type == \"train\":\n",
    "            # self.df = self.df.sample(100).reset_index(drop=True)\n",
    "        # self.max_seq_length = max_seq_length\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.type = type\n",
    "        self.is_augement = is_augement\n",
    "        if self.is_augement:\n",
    "            self.smw = Similarword(create_num=2, change_rate=0.2)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        content = self.df.loc[index, \"sentence\"]\n",
    "        if self.type == \"train\" and self.is_augement:\n",
    "            if random.random() > 0.5:\n",
    "                content = self.smw.replace(content)[-1]\n",
    "        label = self.df.loc[index, \"label\"]\n",
    "        # d_encode = self.tokenizer.encode_plus(content)\n",
    "        # padding\n",
    "        # d_encode = self.tokenizer.encode_plus(content,\n",
    "        #                                       padding=\"max_length\",\n",
    "        #                                       max_length=self.max_seq_length,\n",
    "        #                                       truncation=True)\n",
    "\n",
    "        # sample = {\"input_ids\": d_encode['input_ids'],\n",
    "        #     \"token_type_ids\": d_encode['token_type_ids'],\n",
    "        #     \"attention_mask\": d_encode['attention_mask'],\n",
    "        #     \"length\" : sum(d_encode['attention_mask']),\n",
    "        #     \"label\": label}\n",
    "        return content, label\n",
    "\n",
    "def sup_collate_fn(batch):\n",
    "    texts, labels = [], []\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    for item in batch:\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "    inputs = tokenizer(texts, max_length=40, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "class CLS_model(nn.Module):\n",
    "    def __init__(self, embedding_dim, target_size):\n",
    "        super(CLS_model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # 根据连接后的平均和最大池化输出的维度来定义fc1,增加的全连接层和Dropout\n",
    "        self.fc1 = nn.Linear(768 * 2, embedding_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)  \n",
    "        self.fc2 = nn.Linear(embedding_dim, embedding_dim // 2)  \n",
    "        self.fc3 = nn.Linear(embedding_dim // 2, target_size)  \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        output = self.bert(input_ids=input_ids,\n",
    "                           token_type_ids=token_type_ids,\n",
    "                           attention_mask=attention_mask)\n",
    "        \n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        pooled_output = output.pooler_output\n",
    "\n",
    "        # 计算序列的平均和最大值\n",
    "        seq_avg = torch.mean(last_hidden_state, dim=1)\n",
    "        seq_max = torch.max(last_hidden_state, dim=1)[0]\n",
    "        concat_out = torch.cat((seq_avg, seq_max), dim=1)\n",
    "\n",
    "        # 通过全连接层处理\n",
    "        fc1_out = self.dropout(self.activation(self.fc1(concat_out)))\n",
    "        fc2_out = self.dropout(self.activation(self.fc2(fc1_out)))\n",
    "        fc3_out = self.fc3(fc2_out)  \n",
    "\n",
    "        return fc3_out\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model,emb_name,epsilon=1.0):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.emb_name = emb_name\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm!=0 and not torch.isnan(norm):\n",
    "                    r_at = self.epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and self.emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer,fgm):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        inputs = {k: v.to(device) for k, v in data.items() if k != 'labels'}\n",
    "        labels = data['labels'].to(device)\n",
    "\n",
    "        # 正常的前向传播\n",
    "        pred = model(**inputs)\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # 正常的反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # 对抗训练\n",
    "        fgm.attack()\n",
    "        pred_adv = model(**inputs)\n",
    "        loss_adv = loss_fn(pred_adv, labels)\n",
    "        loss_adv.backward()\n",
    "        fgm.restore()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predictions = torch.max(pred, 1)\n",
    "        total_accuracy += (predictions == labels).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            current_loss = total_loss / (batch + 1)\n",
    "            current_accuracy = total_accuracy / ((batch + 1) * dataloader.batch_size)\n",
    "            print(f\"loss: {current_loss:>7f}  [accuracy: {100*current_accuracy:>0.2f}%]\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_accuracy = total_accuracy / size\n",
    "    print(f\"Average loss: {avg_loss:>7f}  [Average accuracy: {100*avg_accuracy:>0.2f}%]\")\n",
    "\n",
    "    \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    with torch.no_grad():\n",
    "        for batch, data in enumerate(dataloader):\n",
    "            inputs = {k: v.to(device) for k, v in data.items() if k != 'labels'}\n",
    "            labels = data['labels']\n",
    "\n",
    "            pred = model(**inputs)\n",
    "            test_loss += loss_fn(pred, labels).item()\n",
    "            _, predictions = torch.max(pred, 1)\n",
    "            correct += (predictions == labels).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    print(f\"Test Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = MyDataset(csv_path = 'dataset/train.csv', type=\"train\", is_augement=False)\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=sup_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101,  100,  100,  ...,    0,    0,    0],\n",
       "        [ 101,  100, 1964,  ...,    0,    0,    0],\n",
       "        [ 101,  100, 1820,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
       "        [ 101,  100,  100,  ...,    0,    0,    0],\n",
       "        [ 101,  100,  100,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([ 7,  3,  5,  7, 10,  2,  8,  7,  8, 10,  5,  3,  7,  8,  5,  0,  7,  5,\n",
       "         7,  6, 12,  4,  2, 12,  3, 13,  7,  1,  5,  3,  5,  3])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(trainloader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLS_model(embedding_dim=200, target_size=15)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "fgm = FGM(model, epsilon=1, emb_name='bert.embeddings.word_embeddings.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.687459  [accuracy: 9.38%]\n",
      "loss: 2.631022  [accuracy: 9.81%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# test_loop(test_loader, model, loss_fn)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, fgm)\u001b[0m\n\u001b[0;32m     20\u001b[0m pred_adv \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     21\u001b[0m loss_adv \u001b[38;5;241m=\u001b[39m loss_fn(pred_adv, labels)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss_adv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m fgm\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\18496\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18496\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train_loop(trainloader, model, loss_fn, optimizer,fgm)\n",
    "    # test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
